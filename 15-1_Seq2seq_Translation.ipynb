{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assumed-affair",
   "metadata": {},
   "source": [
    "# 15-1 시퀀스-투-시퀀스(Sequence-to-Sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-mineral",
   "metadata": {},
   "source": [
    "Input -> Encoder -> CONTEXT Vector -> Decoder -> Output 의 구조 <br>\n",
    "encoder, decoder은 각각이 RNN 구조. (LSTM, GRU) <br>\n",
    "디코더의 초기 입력 = < sos > 최종 출력 = < eos >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "focal-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fatal-escape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189986"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t') # source, target\n",
    "del lines['lic']\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "charming-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "expressed-zambia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48447</th>\n",
       "      <td>I remember the letter.</td>\n",
       "      <td>Je me souviens de cette lettre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Get out.</td>\n",
       "      <td>Décampez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19903</th>\n",
       "      <td>The bird is dead.</td>\n",
       "      <td>L'oiseau est mort.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11783</th>\n",
       "      <td>Tom is similar.</td>\n",
       "      <td>Tom est pareil.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44101</th>\n",
       "      <td>This is the best one.</td>\n",
       "      <td>Il s’agit du meilleur.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53604</th>\n",
       "      <td>Don't you speak French?</td>\n",
       "      <td>Ne parlez-vous pas français ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32708</th>\n",
       "      <td>You cannot do this.</td>\n",
       "      <td>Tu ne peux pas faire cela.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37679</th>\n",
       "      <td>Tom is somewhat shy.</td>\n",
       "      <td>Tom est quelque peu timide.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28501</th>\n",
       "      <td>How is your mother?</td>\n",
       "      <td>Comment va votre mère ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13351</th>\n",
       "      <td>He's a slowpoke.</td>\n",
       "      <td>C'est un lambin.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                              tar\n",
       "48447   I remember the letter.  Je me souviens de cette lettre.\n",
       "249                   Get out.                       Décampez !\n",
       "19903        The bird is dead.               L'oiseau est mort.\n",
       "11783          Tom is similar.                  Tom est pareil.\n",
       "44101    This is the best one.           Il s’agit du meilleur.\n",
       "53604  Don't you speak French?    Ne parlez-vous pas français ?\n",
       "32708      You cannot do this.       Tu ne peux pas faire cela.\n",
       "37679     Tom is somewhat shy.      Tom est quelque peu timide.\n",
       "28501      How is your mother?          Comment va votre mère ?\n",
       "13351         He's a slowpoke.                 C'est un lambin."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-pulse",
   "metadata": {},
   "source": [
    "번역 문장에 해당하는 프랑스어 데이터는 < sos >, < eos > 심볼을 넣어준다 <br>\n",
    "< sos > -> \\t <br>\n",
    "< eos > -> \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "urban-violin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45859</th>\n",
       "      <td>You're overemotional.</td>\n",
       "      <td>\\tVous vous laissez trop dominer par vos émoti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23045</th>\n",
       "      <td>I didn't kiss Tom.</td>\n",
       "      <td>\\tJe n'ai pas embrassé Tom.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31460</th>\n",
       "      <td>Tom can't use this.</td>\n",
       "      <td>\\tTom ne peut pas utiliser ceci.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>I was upstairs.</td>\n",
       "      <td>\\tJ'étais en haut.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57399</th>\n",
       "      <td>The day is almost over.</td>\n",
       "      <td>\\tLa journée tire à sa fin.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14254</th>\n",
       "      <td>I'm a happy man.</td>\n",
       "      <td>\\tJe suis un homme heureux.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54893</th>\n",
       "      <td>I hope we achieve that.</td>\n",
       "      <td>\\tJ'espère que nous finirons cela.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27661</th>\n",
       "      <td>Do you accept Visa?</td>\n",
       "      <td>\\tVous prenez la VISA ?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>I was drugged.</td>\n",
       "      <td>\\tJ'ai été droguée.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27810</th>\n",
       "      <td>Don't make a sound.</td>\n",
       "      <td>\\tNe faites pas un bruit !\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src  \\\n",
       "45859    You're overemotional.   \n",
       "23045       I didn't kiss Tom.   \n",
       "31460      Tom can't use this.   \n",
       "10501          I was upstairs.   \n",
       "57399  The day is almost over.   \n",
       "14254         I'm a happy man.   \n",
       "54893  I hope we achieve that.   \n",
       "27661      Do you accept Visa?   \n",
       "7368            I was drugged.   \n",
       "27810      Don't make a sound.   \n",
       "\n",
       "                                                     tar  \n",
       "45859  \\tVous vous laissez trop dominer par vos émoti...  \n",
       "23045                      \\tJe n'ai pas embrassé Tom.\\n  \n",
       "31460                 \\tTom ne peut pas utiliser ceci.\\n  \n",
       "10501                               \\tJ'étais en haut.\\n  \n",
       "57399                      \\tLa journée tire à sa fin.\\n  \n",
       "14254                      \\tJe suis un homme heureux.\\n  \n",
       "54893               \\tJ'espère que nous finirons cela.\\n  \n",
       "27661                          \\tVous prenez la VISA ?\\n  \n",
       "7368                               \\tJ'ai été droguée.\\n  \n",
       "27810                       \\tNe faites pas un bruit !\\n  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x: '\\t'+x+'\\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "concerned-controversy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "# src 글자 집합\n",
    "src_vocab = set()\n",
    "for line in lines.src:\n",
    "    for char in line:\n",
    "        src_vocab.add(char)\n",
    "\n",
    "# tar 글자 집합\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)\n",
    "\n",
    "print(len(src_vocab)+1) # src_vocab 크기\n",
    "print(len(tar_vocab)+1) # tar_vocab 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "colored-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정렬\n",
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "\n",
    "# 인덱스 부여\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "serial-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 문장 정수 인코딩\n",
    "encoder_input = []\n",
    "for line in lines.src:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(src_to_index[w]) # 글자에 해당하는 정수\n",
    "    encoder_input.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "retired-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프랑스어 문장 정수 인코딩\n",
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w]) # 글자에 해당하는 정수\n",
    "    decoder_input.append(temp_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-invention",
   "metadata": {},
   "source": [
    "디코더의 예측값과 비교하기 위한 실제값을 알려주어야 함.\n",
    "실제값은 < sos >가 없으므로 정수 인코딩에서 < sos >를 삭제함 (\\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "organic-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <sos> 제거. 맨앞 제거하면 됨\n",
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t = 0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        if t>0:\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t += 1\n",
    "    decoder_target.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "loving-retreat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 48, 53, 3, 4, 2]\n",
      "[48, 53, 3, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[0])\n",
    "print(decoder_target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "through-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aging-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-money",
   "metadata": {},
   "source": [
    "## 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "amazing-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "whole-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None, len(src_vocab)+1))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs) # encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c] # LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태.\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, len(tar_vocab)+1))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states) # encoder의 state를 받음)\n",
    "\n",
    "decoder_softmax_layer = Dense(len(tar_vocab)+1, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "elect-cuisine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.7483 - val_loss: 0.6768\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.4663 - val_loss: 0.5426\n",
      "Epoch 3/50\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.3912 - val_loss: 0.4799\n",
      "Epoch 4/50\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.3485 - val_loss: 0.4475\n",
      "Epoch 5/50\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.3200 - val_loss: 0.4198\n",
      "Epoch 6/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.2988 - val_loss: 0.4020\n",
      "Epoch 7/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.2824 - val_loss: 0.3912\n",
      "Epoch 8/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.2692 - val_loss: 0.3810\n",
      "Epoch 9/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.2582 - val_loss: 0.3753\n",
      "Epoch 10/50\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.2486 - val_loss: 0.3690\n",
      "Epoch 11/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.2402 - val_loss: 0.3653\n",
      "Epoch 12/50\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.2328 - val_loss: 0.3630\n",
      "Epoch 13/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.2260 - val_loss: 0.3608\n",
      "Epoch 14/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.2199 - val_loss: 0.3597\n",
      "Epoch 15/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.2142 - val_loss: 0.3590\n",
      "Epoch 16/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.2090 - val_loss: 0.3578\n",
      "Epoch 17/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.2041 - val_loss: 0.3590\n",
      "Epoch 18/50\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.1996 - val_loss: 0.3578\n",
      "Epoch 19/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1953 - val_loss: 0.3599\n",
      "Epoch 20/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1912 - val_loss: 0.3622\n",
      "Epoch 21/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1874 - val_loss: 0.3629\n",
      "Epoch 22/50\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.1838 - val_loss: 0.3644\n",
      "Epoch 23/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1803 - val_loss: 0.3658\n",
      "Epoch 24/50\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.1771 - val_loss: 0.3674\n",
      "Epoch 25/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1741 - val_loss: 0.3685\n",
      "Epoch 26/50\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.1710 - val_loss: 0.3717\n",
      "Epoch 27/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1682 - val_loss: 0.3722\n",
      "Epoch 28/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1656 - val_loss: 0.3764\n",
      "Epoch 29/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1629 - val_loss: 0.3797\n",
      "Epoch 30/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1606 - val_loss: 0.3801\n",
      "Epoch 31/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1580 - val_loss: 0.3840\n",
      "Epoch 32/50\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.1559 - val_loss: 0.3866\n",
      "Epoch 33/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1536 - val_loss: 0.3892\n",
      "Epoch 34/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1515 - val_loss: 0.3920\n",
      "Epoch 35/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1495 - val_loss: 0.3947\n",
      "Epoch 36/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1475 - val_loss: 0.3970\n",
      "Epoch 37/50\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.1456 - val_loss: 0.3999\n",
      "Epoch 38/50\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.1439 - val_loss: 0.4022\n",
      "Epoch 39/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1420 - val_loss: 0.4037\n",
      "Epoch 40/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1405 - val_loss: 0.4078\n",
      "Epoch 41/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1388 - val_loss: 0.4122\n",
      "Epoch 42/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1373 - val_loss: 0.4123\n",
      "Epoch 43/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1357 - val_loss: 0.4162\n",
      "Epoch 44/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1342 - val_loss: 0.4203\n",
      "Epoch 45/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1327 - val_loss: 0.4196\n",
      "Epoch 46/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1313 - val_loss: 0.4253\n",
      "Epoch 47/50\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.1300 - val_loss: 0.4258\n",
      "Epoch 48/50\n",
      "750/750 [==============================] - 9s 11ms/step - loss: 0.1287 - val_loss: 0.4304\n",
      "Epoch 49/50\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.1274 - val_loss: 0.4319\n",
      "Epoch 50/50\n",
      "750/750 [==============================] - 9s 12ms/step - loss: 0.1262 - val_loss: 0.4355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f83d5be1d60>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2) # 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-cruise",
   "metadata": {},
   "source": [
    "## Seq2seq 기계 번역 동작\n",
    "1. 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻음 <br>\n",
    "2. 두 상태와 < sos >에 해당하는 '\\t'를 디코더로 보냄. <br>\n",
    "3. 디코더가 < eos >에 해당하는 '\\n'이 나올 때까지 다음 문자를 예측하는 행동을 반복함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "racial-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pregnant-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "smooth-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    tar_vocab_size = len(tar_vocab)+1\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "medium-stage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Hi.\n",
      "정답 문장: Salut !\n",
      "번역기가 번역한 문장: Salut.\n",
      "-----------------------------------\n",
      "입력 문장: I see.\n",
      "정답 문장: Aha.\n",
      "번역기가 번역한 문장: Je vous en allement.\n",
      "-----------------------------------\n",
      "입력 문장: Hug me.\n",
      "정답 문장: Serrez-moi dans vos bras !\n",
      "번역기가 번역한 문장: Serre-moi dans tes bras !\n",
      "-----------------------------------\n",
      "입력 문장: Hold it!\n",
      "정답 문장: Restez où vous êtes !\n",
      "번역기가 번역한 문장: Ne bougez plus !\n",
      "-----------------------------------\n",
      "입력 문장: I crashed.\n",
      "정답 문장: Je me suis écrasée.\n",
      "번역기가 번역한 문장: Je me suis émoré.\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-produce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
