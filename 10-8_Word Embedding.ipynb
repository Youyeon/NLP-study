{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556c285d",
   "metadata": {},
   "source": [
    "# 10-8 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b10ada1",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed91bc",
   "metadata": {},
   "source": [
    "## 1. 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2f614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3881a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y = [1,0,0,1,1,0,1] # 1은 긍정, 0은 부정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a710c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)\n",
    "vocab_size = len(t.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87383af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수 인코딩\n",
    "X_encoded = t.texts_to_sequences(sentences)\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "196c4939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max(len(l) for l in X_encoded)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55d3d89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4],\n",
       "       [ 5,  6,  0,  0],\n",
       "       [ 7,  8,  0,  0],\n",
       "       [ 9, 10,  0,  0],\n",
       "       [11, 12,  0,  0],\n",
       "       [13,  0,  0,  0],\n",
       "       [14, 15,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 패딩\n",
    "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
    "y_train = np.array(y)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b73ce34",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7409e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 0s - loss: 0.6966 - acc: 0.5714\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.6950 - acc: 0.5714\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.6933 - acc: 0.5714\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.6916 - acc: 0.5714\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.6899 - acc: 0.5714\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.6883 - acc: 0.5714\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.6866 - acc: 0.5714\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.6849 - acc: 0.5714\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.6833 - acc: 0.5714\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.6816 - acc: 0.5714\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.6799 - acc: 0.5714\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.6783 - acc: 0.5714\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.6766 - acc: 0.5714\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.6750 - acc: 0.5714\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.6733 - acc: 0.7143\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.6717 - acc: 0.7143\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.6700 - acc: 0.8571\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.6683 - acc: 0.8571\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.6667 - acc: 0.8571\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.6650 - acc: 0.8571\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.6634 - acc: 0.8571\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.6617 - acc: 0.8571\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.6600 - acc: 0.8571\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.6583 - acc: 0.8571\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.6567 - acc: 0.8571\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.6550 - acc: 0.8571\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.6533 - acc: 0.8571\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.6516 - acc: 0.8571\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.6499 - acc: 0.8571\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.6482 - acc: 0.8571\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.6464 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.6447 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.6430 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.6413 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.6395 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.6378 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.6360 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.6342 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.6325 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.6307 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.6289 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.6271 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.6253 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.6235 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.6217 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.6198 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.6180 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.6161 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.6143 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.6124 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.6106 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.6087 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.6068 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.6049 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.6030 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.6011 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.5992 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.5973 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.5954 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.5934 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.5915 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.5895 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.5876 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.5856 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.5836 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.5816 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.5797 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.5777 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.5757 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.5736 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.5716 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.5696 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.5676 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.5656 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.5635 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.5615 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.5594 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.5574 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.5553 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.5532 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.5511 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.5491 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.5470 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.5449 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.5428 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.5407 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.5386 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.5365 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.5344 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.5322 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.5301 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.5280 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.5259 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.5237 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.5216 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.5194 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.5173 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.5151 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.5130 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.5108 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f173c0b1190>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 4, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9d7e2",
   "metadata": {},
   "source": [
    "## 2-2 전처리된 레이어 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60d74cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000개의 Embedding vector가 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embedding_dict = dict()\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0] # 단어\n",
    "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 임베딩 벡터: 100개의 값을 가지는 array로 변환\n",
    "    embedding_dict[word] = word_vector_arr\n",
    "f.close()\n",
    "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dc9aff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5999d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in t.word_index.items(): # 훈련 데이터의 단어 집합에서 단어를 1개씩 꺼내온다.\n",
    "    temp = embedding_dict.get(word) # 단어(key) 해당되는 임베딩 벡터의 100개의 값(value)를 임시 변수에 저장\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp # 임수 변수의 값을 단어와 매핑되는 인덱스의 행에 삽입"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9cd55f",
   "metadata": {},
   "source": [
    "## Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55770f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 0s - loss: 0.8060 - acc: 0.2857\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.7804 - acc: 0.2857\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.7556 - acc: 0.2857\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.7316 - acc: 0.4286\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.7084 - acc: 0.5714\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.6861 - acc: 0.7143\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.6648 - acc: 0.7143\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.6443 - acc: 0.7143\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.6247 - acc: 0.8571\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.6059 - acc: 0.8571\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.5881 - acc: 0.8571\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.5710 - acc: 0.8571\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.5548 - acc: 0.8571\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.5393 - acc: 0.8571\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.5245 - acc: 0.8571\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.5104 - acc: 0.8571\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.4969 - acc: 0.8571\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.4841 - acc: 0.8571\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.4717 - acc: 0.8571\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.4599 - acc: 0.8571\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.4486 - acc: 0.8571\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.4377 - acc: 0.8571\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.4272 - acc: 0.8571\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.4170 - acc: 0.8571\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.4073 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.3978 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.3887 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.3799 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.3714 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.3631 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.3551 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.3473 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.3397 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.3324 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.3253 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.3184 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.3117 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.3051 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.2988 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.2926 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.2867 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.2808 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.2752 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.2697 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.2643 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.2591 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.2541 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.2492 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.2444 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.2397 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.2352 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.2308 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.2266 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.2224 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.2183 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.2144 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.2106 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.2068 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.2032 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.1997 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.1962 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.1929 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.1896 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.1864 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.1833 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.1803 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.1773 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.1745 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.1717 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.1689 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.1663 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.1637 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.1611 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.1586 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.1562 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.1538 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.1515 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.1493 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.1471 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.1449 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.1428 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.1408 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.1388 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.1368 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.1349 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.1330 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.1311 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.1293 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.1276 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.1259 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.1242 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.1225 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.1209 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.1193 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.1178 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.1162 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.1148 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.1133 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.1119 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.1105 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f16061e34c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False) # trainable=False: 학습X\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07def2",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5f1f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글의 사전 훈련된 Word2vec 모델을 로드합니다.\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)  \n",
    "print(word2vec_model.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "for word, i in t.word_index.items(): # 훈련 데이터의 단어 집합에서 단어와 정수 인덱스를 1개씩 꺼내온다.\n",
    "    temp = get_vector(word) # 단어(key) 해당되는 임베딩 벡터의 300개의 값(value)를 임시 변수에 저장\n",
    "    if temp is not None: # 만약 None이 아니라면 임베딩 벡터의 값을 리턴받은 것이므로\n",
    "        embedding_matrix[i] = temp # 해당 단어 위치의 행에 벡터의 값을 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405fb4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
